{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../Datasets/full_data_january.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-830da67ebc25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmonth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"january\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Datasets/full_data_january.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../Datasets/full_data_january.csv' does not exist"
     ]
    }
   ],
   "source": [
    "month = \"january\"\n",
    "df = pd.read_csv(\"../Datasets/full_data_january.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts_and_Entertainment\n",
      "Travel\n",
      "Games\n",
      "Science\n",
      "Business_and_Industry\n",
      "Books_and_Literature\n",
      "Career_and_Education\n",
      "Adult\n",
      "Reference\n",
      "Beauty_and_Fitness\n",
      "People_and_Society\n",
      "Home_and_Garden\n",
      "Health\n",
      "Finance\n",
      "Autos_and_Vehicles\n",
      "Internet_and_Telecom\n",
      "Sports\n",
      "Shopping\n",
      "Food_and_Drink\n",
      "Pets_and_Animals\n",
      "Computer_and_Electronics\n",
      "Law_and_Government\n",
      "News_and_Media\n",
      "Recreation_and_Hobbies\n",
      "Gambling\n"
     ]
    }
   ],
   "source": [
    "top = 2500\n",
    "words_frequency = {}\n",
    "for category in set(df['main_category'].values):\n",
    "    print(category)\n",
    "    all_words = []\n",
    "    for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "        for word in ast.literal_eval(row):\n",
    "            all_words.append(word)\n",
    "    most_common = nltk.FreqDist(w for w in all_words).most_common(top)\n",
    "    words_frequency[category] = most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in set(df['main_category'].values):\n",
    "    words_frequency[category] = [word for word, number in words_frequency[category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Score:  0.8547826086956521\n",
      "Top:  2500\n",
      "Dataset length:  10452\n",
      "\n",
      "SVM\n",
      "Score:  0.7884057971014493\n",
      "Top:  2500\n",
      "Dataset length:  10452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Dataset length: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "from sklearn.externals import joblib\n",
    "filename = \"../Models/{}/LR_model_{}.joblib\".format(month.title(), month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(lr, filename)\n",
    "    \n",
    "filename = \"../Models/{}/LSVM_model_{}.joblib\".format(month.title(), month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(clf, filename)\n",
    "\n",
    "import pickle\n",
    "words_filename = \"../Models/{}/word_frequency_{}.picle\".format(month.title(), month)\n",
    "if not os.path.isfile(words_filename):\n",
    "    pickle_out = open(words_filename,\"wb\")\n",
    "    pickle.dump(words_frequency, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"../Models/{}/word_frequency_{}.picle\".format(month.title(), month),\"rb\")\n",
    "words_frequency_2 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "temp = copy.deepcopy(words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = copy.deepcopy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove chunk words\n",
    "# from math import floor\n",
    "# words = []\n",
    "# for category in words_frequency.keys():\n",
    "#     words.extend(words_frequency[category][0:20])\n",
    "# words_counter = Counter(words)\n",
    "# chunk_words = [x for x in words_counter if words_counter[x] >= 12]\n",
    "# words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 12}\n",
    "# for cat in words_frequency.keys():\n",
    "#     words_frequency[cat] = [word for word in words_frequency[cat] if word not in chunk_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = pd.read_csv(\"../Datasets/websites_custom.csv\")[['URL', 'Category']]\n",
    "df_custom = df_custom[df_custom['URL'].notnull()][:20]\n",
    "df_custom['Weight_model'] = ''\n",
    "df_custom['lr_normal'] = ''\n",
    "df_custom['lr_max'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://www.bbc.com/ . 1 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.telegraph.co.uk/news/ . 2 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://news.sky.com/world . 3 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.thesun.co.uk/news/ . 4 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.dailystar.co.uk/news . 5 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.express.co.uk/news . 6 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.theguardian.com/world . 7 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://globalnews.ca/world/ . 8 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://edition.cnn.com/ . 9 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  Internet_and_Telecom\n",
      "LR normal:  Internet_and_Telecom\n",
      "url: https://news.google.com/?hl=en-US&gl=US&ceid=US:en . 10 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.mirror.co.uk/news/ . 11 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.standard.co.uk/news . 12 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.independent.ie/news/ . 13 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://abcnews.go.com/International/wireStory/argentina-submarine-found-sunk-year-disappearing-59259965 . 14 / 20\n",
      "Category:  News_and_Media\n",
      "My model:  News_and_Media\n",
      "LR normal:  News_and_Media\n",
      "url: https://www.ebay.com/ . 15 / 20\n",
      "Category:  Shopping\n",
      "My model:  Shopping\n",
      "LR normal:  Shopping\n",
      "url: https://www.amazon.com/ . 16 / 20\n",
      "Category:  Shopping\n",
      "My model:  Shopping\n",
      "LR normal:  Shopping\n",
      "url: https://www.aliexpress.com/ . 17 / 20\n",
      "Category:  Shopping\n",
      "My model:  Shopping\n",
      "LR normal:  Shopping\n",
      "url: https://www.alibaba.com/ . 18 / 20\n",
      "Category:  Shopping\n",
      "My model:  Business_and_Industry\n",
      "LR normal:  Business_and_Industry\n",
      "url: https://www.banggood.com/ . 19 / 20\n",
      "Category:  Shopping\n",
      "My model:  Shopping\n",
      "LR normal:  Shopping\n",
      "url: https://www.dx.com/ . 20 / 20\n",
      "Category:  Shopping\n",
      "My model:  Shopping\n",
      "LR normal:  Travel\n"
     ]
    }
   ],
   "source": [
    "top = 2500\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "row_counter = 0 \n",
    "for row_id, row in df_custom.iterrows():\n",
    "    row_counter += 1\n",
    "    try:\n",
    "        url = row['URL']\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        html = urlopen(req, timeout=15).read()\n",
    "        # html = urlopen(url, timeout=15).read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        [tag.decompose() for tag in soup(\"script\")]\n",
    "        [tag.decompose() for tag in soup(\"style\")]\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        # Tokenize text\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        # Remove stopwords\n",
    "        wnl = WordNetLemmatizer()\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        tokens_stopwords = [w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist]\n",
    "        tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_stopwords]\n",
    "        \n",
    "        from collections import Counter\n",
    "        counter = 0\n",
    "        features_pred = np.zeros(top * len(words_frequency)).reshape(len(words_frequency), top)\n",
    "        c = [word for word, word_count in Counter(tokens_lemmatize).most_common(top)]\n",
    "        for category in words_frequency.keys():\n",
    "            for word in c:\n",
    "                if word in words_frequency[category]:\n",
    "                    features_pred[counter][words_frequency[category].index(word)] = 1\n",
    "            counter+=1\n",
    "\n",
    "        category_weight = []\n",
    "        for i in features_pred:\n",
    "            weight_cof = np.where(i == 1)[0]\n",
    "            weight_sum = 0\n",
    "            for cof in weight_cof:\n",
    "                weight_sum += top - cof\n",
    "            category_weight.append(weight_sum)\n",
    "\n",
    "        cat_index = category_weight.index(max(category_weight))\n",
    "        category = list(words_frequency.keys())[cat_index]\n",
    "        feature = features_pred[cat_index].reshape(-1, top)\n",
    "        print(\"url: {} . {} / {}\".format(url, row_counter, len(df_custom)))\n",
    "        print('Category: ', row['Category'])\n",
    "        print(\"My model: \",category)\n",
    "        prediction = lr.predict(feature)\n",
    "        print(\"LR normal: \", list(words_frequency.keys())[int(prediction[0])])\n",
    "        df_custom.at[row_id, 'Weight_model'] = category\n",
    "        df_custom.at[row_id, 'lr_normal'] = list(words_frequency.keys())[int(prediction[0])]\n",
    "    except:\n",
    "        print(\"{} - Failed. {} / {}\".format(row['URL'], row_counter, len(df_custom)))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = df_custom[df_custom['Weight_model'] != '']\n",
    "model_acc = len(df_custom[df_custom['Weight_model'] == df_custom['Category']]) / len(df_custom) * 100\n",
    "lr_acc = len(df_custom[df_custom['lr_normal'] == df_custom['Category']]) / len(df_custom) * 100\n",
    "print(\"My model accuracy: {}% | {} / {}\".format(model_acc, len(df_custom[df_custom['Weight_model'] == df_custom['Category']]), len(df_custom)))\n",
    "print(\"Logistic regression accuracy: {}% | {} / {}\".format(lr_acc, len(df_custom[df_custom['lr_normal'] == df_custom['Category']]), len(df_custom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in list(set(df_custom['Category'].values)):\n",
    "    print(category)\n",
    "    df2 = df_custom[df_custom['Category'] == category]\n",
    "    result_w = len(df2[df2['Weight_model'] == category])\n",
    "    result_l = len(df2[df2['lr_normal'] == category])   \n",
    "    print(\"Model: {} / {} : {:.2f}%\".format(result_w, len(df2), result_w / len(df2) * 100))\n",
    "    print(\"lr: {} / {} : {:.2f}%\".format(result_l, len(df2), result_l / len(df2) * 100))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
